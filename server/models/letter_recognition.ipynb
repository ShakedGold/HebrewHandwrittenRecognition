{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### load the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "### path = data/dataset\n",
    "from PIL import Image\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### method to convert img to arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function destroyAllWindows>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import random\n",
    "\n",
    "def modify_image(img):\n",
    "    # Convert to PIL Image for easier rotation\n",
    "    pil_img = Image.fromarray(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "    \n",
    "    # 1. Random angle rotation\n",
    "    angle = random.uniform(-30, 30)\n",
    "    rotated = pil_img.rotate(angle, expand=True, fillcolor=(255, 255, 255))\n",
    "    \n",
    "    # 2. Off-center shift\n",
    "    shift_x = random.randint(-20, 20)\n",
    "    shift_y = random.randint(-20, 20)\n",
    "    \n",
    "    # Create a new image with a white background\n",
    "    new_img = Image.new('RGB', (rotated.width + abs(shift_x), rotated.height + abs(shift_y)), (255, 255, 255))\n",
    "    \n",
    "    # Paste the rotated image onto the new image with the shift\n",
    "    new_img.paste(rotated, (max(shift_x, 0), max(shift_y, 0)))\n",
    "    \n",
    "    # Convert back to OpenCV format\n",
    "    cv_img = cv2.cvtColor(np.array(new_img), cv2.COLOR_RGB2BGR)\n",
    "    \n",
    "    # 3. Add noise\n",
    "    # noise = np.random.normal(0, 10, cv_img.shape).astype(np.uint8)\n",
    "    # noisy_img = cv2.add(cv_img, noise)\n",
    "    \n",
    "    return cv_img\n",
    "\n",
    "# Example usage:\n",
    "img = Image.open('../data/dataset/TRAIN/5/5_5.png')\n",
    "modified_img = modify_image(np.array(img))\n",
    "#show image\n",
    "cv2.imshow('image', modified_img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "def convert_img_to_binary_array(img):\n",
    "    #convert image to rgba\n",
    "    img = img.resize((40, 40))  \n",
    "    img = img.convert('RGBA')\n",
    "\n",
    "    #get the pixel values of the image\n",
    "    pixels = list(img.getdata())\n",
    "    binary_array = []\n",
    "    for pixel in pixels:\n",
    "        if pixel[0] >= 128 and pixel[1] >= 128 and pixel[2] >= 128:\n",
    "            binary_array.append(1)\n",
    "        else:\n",
    "            binary_array.append(0)\n",
    "    return binary_array\n",
    "\n",
    "def add_image_to_X_train(image, X_train):\n",
    "    # image = modify_image(image)\n",
    "    # print(image.shape)\n",
    "\n",
    "    # Resize the image to 40x40 if it's not already\n",
    "    if image.shape[:2] != (40, 40):\n",
    "        image = cv2.resize(image, (40, 40), interpolation=cv2.INTER_AREA)\n",
    "    \n",
    "    # Convert to grayscale if the image has more than one channel\n",
    "    if len(image.shape) > 2:\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Convert to black and white (0 or 255)\n",
    "    _, bw_image = cv2.threshold(image, 127, 255, cv2.THRESH_BINARY)\n",
    "    \n",
    "    # Normalize to 0 or 1 for the neural network\n",
    "    normalized_image = (bw_image / 255).astype(np.float32)\n",
    "    \n",
    "    # Reshape to add channel dimension\n",
    "    reshaped_image = normalized_image.reshape(1, 40, 40, 1)\n",
    "\n",
    "    # Add to X_train\n",
    "    if X_train is None:\n",
    "        X_train = reshaped_image\n",
    "    else:\n",
    "        X_train = np.vstack((X_train, reshaped_image))\n",
    "    return X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create training and testing arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing folder: 0\n",
      "Processing folder: 1\n",
      "Processing folder: 10\n",
      "Processing folder: 11\n",
      "Processing folder: 12\n",
      "Processing folder: 13\n",
      "Processing folder: 14\n",
      "Processing folder: 15\n",
      "Processing folder: 16\n",
      "Processing folder: 17\n",
      "Processing folder: 18\n",
      "Processing folder: 19\n",
      "Processing folder: 2\n",
      "Processing folder: 20\n",
      "Processing folder: 21\n",
      "Processing folder: 22\n",
      "Processing folder: 23\n",
      "Processing folder: 24\n",
      "Processing folder: 25\n",
      "Processing folder: 26\n",
      "Processing folder: 3\n",
      "Processing folder: 4\n",
      "Processing folder: 5\n",
      "Processing folder: 6\n",
      "Processing folder: 7\n",
      "Processing folder: 8\n",
      "Processing folder: 9\n",
      "[[[[1.]\n",
      "   [1.]\n",
      "   [1.]\n",
      "   ...\n",
      "   [1.]\n",
      "   [1.]\n",
      "   [1.]]\n",
      "\n",
      "  [[1.]\n",
      "   [1.]\n",
      "   [1.]\n",
      "   ...\n",
      "   [1.]\n",
      "   [1.]\n",
      "   [1.]]\n",
      "\n",
      "  [[1.]\n",
      "   [1.]\n",
      "   [1.]\n",
      "   ...\n",
      "   [1.]\n",
      "   [1.]\n",
      "   [1.]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[1.]\n",
      "   [1.]\n",
      "   [1.]\n",
      "   ...\n",
      "   [1.]\n",
      "   [1.]\n",
      "   [1.]]\n",
      "\n",
      "  [[1.]\n",
      "   [1.]\n",
      "   [1.]\n",
      "   ...\n",
      "   [1.]\n",
      "   [1.]\n",
      "   [1.]]\n",
      "\n",
      "  [[1.]\n",
      "   [1.]\n",
      "   [1.]\n",
      "   ...\n",
      "   [1.]\n",
      "   [1.]\n",
      "   [1.]]]\n",
      "\n",
      "\n",
      " [[[1.]\n",
      "   [1.]\n",
      "   [1.]\n",
      "   ...\n",
      "   [1.]\n",
      "   [1.]\n",
      "   [1.]]\n",
      "\n",
      "  [[1.]\n",
      "   [1.]\n",
      "   [1.]\n",
      "   ...\n",
      "   [1.]\n",
      "   [1.]\n",
      "   [1.]]\n",
      "\n",
      "  [[1.]\n",
      "   [1.]\n",
      "   [1.]\n",
      "   ...\n",
      "   [1.]\n",
      "   [1.]\n",
      "   [1.]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[1.]\n",
      "   [1.]\n",
      "   [1.]\n",
      "   ...\n",
      "   [1.]\n",
      "   [1.]\n",
      "   [1.]]\n",
      "\n",
      "  [[1.]\n",
      "   [1.]\n",
      "   [1.]\n",
      "   ...\n",
      "   [1.]\n",
      "   [1.]\n",
      "   [1.]]\n",
      "\n",
      "  [[1.]\n",
      "   [1.]\n",
      "   [1.]\n",
      "   ...\n",
      "   [1.]\n",
      "   [1.]\n",
      "   [1.]]]\n",
      "\n",
      "\n",
      " [[[1.]\n",
      "   [1.]\n",
      "   [1.]\n",
      "   ...\n",
      "   [1.]\n",
      "   [1.]\n",
      "   [1.]]\n",
      "\n",
      "  [[1.]\n",
      "   [1.]\n",
      "   [1.]\n",
      "   ...\n",
      "   [1.]\n",
      "   [1.]\n",
      "   [1.]]\n",
      "\n",
      "  [[1.]\n",
      "   [1.]\n",
      "   [1.]\n",
      "   ...\n",
      "   [1.]\n",
      "   [1.]\n",
      "   [1.]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[1.]\n",
      "   [1.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [1.]\n",
      "   [1.]\n",
      "   [1.]]\n",
      "\n",
      "  [[1.]\n",
      "   [1.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [1.]\n",
      "   [1.]\n",
      "   [1.]]\n",
      "\n",
      "  [[1.]\n",
      "   [1.]\n",
      "   [1.]\n",
      "   ...\n",
      "   [1.]\n",
      "   [1.]\n",
      "   [1.]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[1.]\n",
      "   [1.]\n",
      "   [1.]\n",
      "   ...\n",
      "   [1.]\n",
      "   [1.]\n",
      "   [1.]]\n",
      "\n",
      "  [[1.]\n",
      "   [1.]\n",
      "   [1.]\n",
      "   ...\n",
      "   [1.]\n",
      "   [1.]\n",
      "   [1.]]\n",
      "\n",
      "  [[1.]\n",
      "   [1.]\n",
      "   [1.]\n",
      "   ...\n",
      "   [1.]\n",
      "   [1.]\n",
      "   [1.]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[1.]\n",
      "   [1.]\n",
      "   [1.]\n",
      "   ...\n",
      "   [1.]\n",
      "   [1.]\n",
      "   [1.]]\n",
      "\n",
      "  [[1.]\n",
      "   [1.]\n",
      "   [1.]\n",
      "   ...\n",
      "   [1.]\n",
      "   [1.]\n",
      "   [1.]]\n",
      "\n",
      "  [[1.]\n",
      "   [1.]\n",
      "   [1.]\n",
      "   ...\n",
      "   [1.]\n",
      "   [1.]\n",
      "   [1.]]]\n",
      "\n",
      "\n",
      " [[[1.]\n",
      "   [1.]\n",
      "   [1.]\n",
      "   ...\n",
      "   [1.]\n",
      "   [1.]\n",
      "   [1.]]\n",
      "\n",
      "  [[1.]\n",
      "   [1.]\n",
      "   [1.]\n",
      "   ...\n",
      "   [1.]\n",
      "   [1.]\n",
      "   [1.]]\n",
      "\n",
      "  [[1.]\n",
      "   [1.]\n",
      "   [1.]\n",
      "   ...\n",
      "   [1.]\n",
      "   [1.]\n",
      "   [1.]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[1.]\n",
      "   [1.]\n",
      "   [1.]\n",
      "   ...\n",
      "   [1.]\n",
      "   [1.]\n",
      "   [1.]]\n",
      "\n",
      "  [[1.]\n",
      "   [1.]\n",
      "   [1.]\n",
      "   ...\n",
      "   [1.]\n",
      "   [1.]\n",
      "   [1.]]\n",
      "\n",
      "  [[1.]\n",
      "   [1.]\n",
      "   [1.]\n",
      "   ...\n",
      "   [1.]\n",
      "   [1.]\n",
      "   [1.]]]\n",
      "\n",
      "\n",
      " [[[1.]\n",
      "   [1.]\n",
      "   [1.]\n",
      "   ...\n",
      "   [1.]\n",
      "   [1.]\n",
      "   [1.]]\n",
      "\n",
      "  [[1.]\n",
      "   [1.]\n",
      "   [1.]\n",
      "   ...\n",
      "   [1.]\n",
      "   [1.]\n",
      "   [1.]]\n",
      "\n",
      "  [[1.]\n",
      "   [1.]\n",
      "   [1.]\n",
      "   ...\n",
      "   [1.]\n",
      "   [1.]\n",
      "   [1.]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[1.]\n",
      "   [1.]\n",
      "   [1.]\n",
      "   ...\n",
      "   [1.]\n",
      "   [1.]\n",
      "   [1.]]\n",
      "\n",
      "  [[1.]\n",
      "   [1.]\n",
      "   [1.]\n",
      "   ...\n",
      "   [1.]\n",
      "   [1.]\n",
      "   [1.]]\n",
      "\n",
      "  [[1.]\n",
      "   [1.]\n",
      "   [1.]\n",
      "   ...\n",
      "   [1.]\n",
      "   [1.]\n",
      "   [1.]]]]\n",
      "3939\n"
     ]
    }
   ],
   "source": [
    "X_train = None\n",
    "y_train = []\n",
    "\n",
    "path = \"../data/dataset/TRAIN/\"\n",
    "for folder in os.listdir(path):\n",
    "    folder_path = os.path.join(path, folder)\n",
    "    if not os.path.isdir(folder_path):\n",
    "        continue\n",
    "    if folder == \"27\":\n",
    "        continue\n",
    "    print(f\"Processing folder: {folder}\")\n",
    "    for file in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, file)\n",
    "        if os.path.isfile(file_path):\n",
    "            try:\n",
    "                img = Image.open(file_path)\n",
    "\n",
    "                X_train = add_image_to_X_train(np.array(img), X_train)\n",
    "\n",
    "                y_train.append(int(folder))\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing file {file_path}: {e}\")\n",
    "\n",
    "print(X_train)\n",
    "print(len(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing folder: 0\n",
      "Processing folder: 1\n",
      "Processing folder: 10\n",
      "Processing folder: 11\n",
      "Processing folder: 12\n",
      "Processing folder: 13\n",
      "Processing folder: 14\n",
      "Processing folder: 15\n",
      "Processing folder: 16\n",
      "Processing folder: 17\n",
      "Processing folder: 18\n",
      "Processing folder: 19\n",
      "Processing folder: 2\n",
      "Processing folder: 20\n",
      "Processing folder: 21\n",
      "Processing folder: 22\n",
      "Processing folder: 23\n",
      "Processing folder: 24\n",
      "Processing folder: 25\n",
      "Processing folder: 26\n",
      "Processing folder: 3\n",
      "Processing folder: 4\n",
      "Processing folder: 5\n",
      "Processing folder: 6\n",
      "Processing folder: 7\n",
      "Processing folder: 8\n",
      "Processing folder: 9\n"
     ]
    }
   ],
   "source": [
    "X_test = None\n",
    "y_test = []\n",
    "path = \"../data/dataset/TEST/\"\n",
    "for folder in os.listdir(path):\n",
    "    folder_path = os.path.join(path, folder)\n",
    "    if not os.path.isdir(folder_path):\n",
    "        continue\n",
    "    if folder == \"27\":\n",
    "        continue\n",
    "    print(f\"Processing folder: {folder}\")\n",
    "    for file in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, file)\n",
    "        if os.path.isfile(file_path):\n",
    "            try:\n",
    "                img = Image.open(file_path)\n",
    "                # binary_array = convert_img_to_binary_array(img)\n",
    "                X_test = add_image_to_X_train(np.array(img), X_test)\n",
    "\n",
    "                y_test.append(int(folder))\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing file {file_path}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## split data randomly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5073\n",
      "5073\n",
      "Train size: 4058\n",
      "Test size: 1015\n",
      "Train size: (4058, 40, 40, 1)\n",
      "Test size: 4058\n"
     ]
    }
   ],
   "source": [
    "X = np.concatenate((X_train, X_test))\n",
    "y = np.array(np.concatenate((y_train, y_test)))\n",
    "\n",
    "print(X.shape[0])\n",
    "print(len(y))\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print(f\"Train size: {len(X_train)}\")\n",
    "print(f\"Test size: {len(X_test)}\")\n",
    "\n",
    "print(f\"Train size: {X_train.shape}\")\n",
    "print(f\"Test size: {len(y_train)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nizan\\Desktop\\letterRec\\.venv\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(40, 40, 1)),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    Flatten(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(27, activation='softmax')  # 27 for 22 regular + 5 final forms\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - accuracy: 0.0744 - loss: 3.1965\n",
      "Epoch 2/20\n",
      "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.3248 - loss: 2.3169\n",
      "Epoch 3/20\n",
      "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.4654 - loss: 1.8043\n",
      "Epoch 4/20\n",
      "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.5326 - loss: 1.5567\n",
      "Epoch 5/20\n",
      "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.5797 - loss: 1.4340\n",
      "Epoch 6/20\n",
      "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.6121 - loss: 1.2863\n",
      "Epoch 7/20\n",
      "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.6350 - loss: 1.2360\n",
      "Epoch 8/20\n",
      "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.6685 - loss: 1.0992\n",
      "Epoch 9/20\n",
      "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.6675 - loss: 1.0957\n",
      "Epoch 10/20\n",
      "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.6921 - loss: 1.0198\n",
      "Epoch 11/20\n",
      "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.6949 - loss: 0.9911\n",
      "Epoch 12/20\n",
      "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.7057 - loss: 0.9590\n",
      "Epoch 13/20\n",
      "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.7242 - loss: 0.8685\n",
      "Epoch 14/20\n",
      "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.7360 - loss: 0.8381\n",
      "Epoch 15/20\n",
      "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.7340 - loss: 0.8240\n",
      "Epoch 16/20\n",
      "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.7464 - loss: 0.7847\n",
      "Epoch 17/20\n",
      "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.7676 - loss: 0.7002\n",
      "Epoch 18/20\n",
      "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.7510 - loss: 0.7106\n",
      "Epoch 19/20\n",
      "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.7748 - loss: 0.6808\n",
      "Epoch 20/20\n",
      "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.7759 - loss: 0.6580\n"
     ]
    }
   ],
   "source": [
    "# convert to one-hot encoding\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "y_train_one_hot = to_categorical(y_train, 27)\n",
    "\n",
    "history = model.fit(X_train, y_train_one_hot, \n",
    "                    epochs=20, \n",
    "                    batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7559 - loss: 1.0214\n",
      "Test accuracy: 0.7694581151008606\n"
     ]
    }
   ],
   "source": [
    "y_test_one_hot = to_categorical(y_test, 27)\n",
    "\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test_one_hot)\n",
    "print(f'Test accuracy: {test_acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "model.save('../models/keras_model.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "First prediction: [8.2747126e-04 1.7023218e-03 9.7002060e-04 1.8449744e-02 4.9260709e-02\n",
      " 4.6229060e-04 2.9227018e-01 1.0048589e-02 3.5537310e-02 3.7968715e-03\n",
      " 9.3293056e-02 1.2245016e-02 2.6188243e-02 1.6550037e-04 3.9607254e-03\n",
      " 8.3831772e-03 1.3979244e-03 2.9636942e-02 1.4408998e-03 8.8915852e-04\n",
      " 8.3066285e-02 2.5245691e-02 2.9384827e-03 6.1563606e-04 1.6039240e-01\n",
      " 1.6071467e-02 1.2074392e-01]\n",
      "Actual: 19\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected a 1D array, got an array with shape (1015, 27)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\nizan\\Desktop\\letterRec\\.venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'predicted'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\nizan\\Desktop\\letterRec\\.venv\\Lib\\site-packages\\pandas\\core\\frame.py:4485\u001b[0m, in \u001b[0;36mDataFrame._set_item_mgr\u001b[1;34m(self, key, value, refs)\u001b[0m\n\u001b[0;32m   4484\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 4485\u001b[0m     loc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_info_axis\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4486\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[0;32m   4487\u001b[0m     \u001b[38;5;66;03m# This item wasn't present, just insert at end\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\nizan\\Desktop\\letterRec\\.venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'predicted'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[50], line 20\u001b[0m\n\u001b[0;32m     18\u001b[0m predictions_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(predictions_dict)\n\u001b[0;32m     19\u001b[0m predictions_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mactual\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m y_test\n\u001b[1;32m---> 20\u001b[0m \u001b[43mpredictions_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpredicted\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m predictions\n\u001b[0;32m     21\u001b[0m predictions_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprobability\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m predictions_df\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: x[x[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpredicted\u001b[39m\u001b[38;5;124m'\u001b[39m]], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     22\u001b[0m predictions_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mactual_probability\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m predictions_df\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: x[x[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mactual\u001b[39m\u001b[38;5;124m'\u001b[39m]], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\nizan\\Desktop\\letterRec\\.venv\\Lib\\site-packages\\pandas\\core\\frame.py:4311\u001b[0m, in \u001b[0;36mDataFrame.__setitem__\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   4308\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setitem_array([key], value)\n\u001b[0;32m   4309\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   4310\u001b[0m     \u001b[38;5;66;03m# set column\u001b[39;00m\n\u001b[1;32m-> 4311\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_set_item\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\nizan\\Desktop\\letterRec\\.venv\\Lib\\site-packages\\pandas\\core\\frame.py:4538\u001b[0m, in \u001b[0;36mDataFrame._set_item\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   4535\u001b[0m             value \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mtile(value, (\u001b[38;5;28mlen\u001b[39m(existing_piece\u001b[38;5;241m.\u001b[39mcolumns), \u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mT\n\u001b[0;32m   4536\u001b[0m             refs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 4538\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_set_item_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrefs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\nizan\\Desktop\\letterRec\\.venv\\Lib\\site-packages\\pandas\\core\\frame.py:4488\u001b[0m, in \u001b[0;36mDataFrame._set_item_mgr\u001b[1;34m(self, key, value, refs)\u001b[0m\n\u001b[0;32m   4485\u001b[0m     loc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info_axis\u001b[38;5;241m.\u001b[39mget_loc(key)\n\u001b[0;32m   4486\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[0;32m   4487\u001b[0m     \u001b[38;5;66;03m# This item wasn't present, just insert at end\u001b[39;00m\n\u001b[1;32m-> 4488\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minsert\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_info_axis\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrefs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4489\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   4490\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iset_item_mgr(loc, value, refs\u001b[38;5;241m=\u001b[39mrefs)\n",
      "File \u001b[1;32mc:\\Users\\nizan\\Desktop\\letterRec\\.venv\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:1370\u001b[0m, in \u001b[0;36mBlockManager.insert\u001b[1;34m(self, loc, item, value, refs)\u001b[0m\n\u001b[0;32m   1368\u001b[0m     value \u001b[38;5;241m=\u001b[39m value\u001b[38;5;241m.\u001b[39mT\n\u001b[0;32m   1369\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(value) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m-> 1370\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1371\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected a 1D array, got an array with shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;241m.\u001b[39mT\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1372\u001b[0m         )\n\u001b[0;32m   1373\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1374\u001b[0m     value \u001b[38;5;241m=\u001b[39m ensure_block_shape(value, ndim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mndim)\n",
      "\u001b[1;31mValueError\u001b[0m: Expected a 1D array, got an array with shape (1015, 27)"
     ]
    }
   ],
   "source": [
    "# prediction\n",
    "predictions = model.predict(X_test)\n",
    "first_prediction = predictions[0]\n",
    "actual = y_test[0]\n",
    "\n",
    "print(f\"First prediction: {first_prediction}\")\n",
    "print(f\"Actual: {actual}\")\n",
    "\n",
    "# convert to DataFrame and show the y_test and if correct\n",
    "predictions_dict = []\n",
    "for i in range(len(predictions)):\n",
    "    prediction = predictions[i]\n",
    "    prediction_dict = {}\n",
    "    for j in range(len(prediction)):\n",
    "        prediction_dict[j] = prediction[j]\n",
    "    predictions_dict.append(prediction_dict)\n",
    "\n",
    "predictions_df = pd.DataFrame(predictions_dict)\n",
    "predictions_df['actual'] = y_test\n",
    "predictions_df['predicted'] = predictions\n",
    "predictions_df['probability'] = predictions_df.apply(lambda x: x[x['predicted']], axis=1)\n",
    "predictions_df['actual_probability'] = predictions_df.apply(lambda x: x[x['actual']], axis=1)\n",
    "predictions_df['correct'] = predictions_df['actual'] == predictions_df['predicted']\n",
    "\n",
    "print(predictions_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a KNN classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_model = KNeighborsClassifier()\n",
    "knn_model.fit(X_train, y_train)\n",
    "knn_predictions = knn_model.predict(X_test)\n",
    "knn_accuracy = accuracy_score(y_test, knn_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Decision Tree classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_model = DecisionTreeClassifier()\n",
    "dt_model.fit(X_train, y_train)\n",
    "dt_predictions = dt_model.predict(X_test)\n",
    "dt_accuracy = accuracy_score(y_test, dt_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Random Forest classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model = RandomForestClassifier()\n",
    "rf_model.fit(X_train, y_train)\n",
    "rf_predictions = rf_model.predict(X_test)\n",
    "rf_accuracy = accuracy_score(y_test, rf_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Naive Bayes classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_model = GaussianNB()\n",
    "nb_model.fit(X_train, y_train)\n",
    "nb_predictions = nb_model.predict(X_test)\n",
    "nb_accuracy = accuracy_score(y_test, nb_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare the accuracies of the models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Accuracy: 0.7497536945812808\n"
     ]
    }
   ],
   "source": [
    "print(\"SVM Accuracy:\", svm_accuracy)\n",
    "# print(\"KNN Accuracy:\", knn_accuracy)\n",
    "# print(\"Decision Tree Accuracy:\", dt_accuracy)\n",
    "# print(\"Random Forest Accuracy:\", rf_accuracy)\n",
    "# print(\"Naive Bayes Accuracy:\", nb_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Algorithm  Accuracy  Precision    Recall  F1-Score\n",
      "0            SVM  0.749754   0.760278  0.749754  0.748954\n",
      "1            KNN  0.566502   0.613491  0.566502  0.556145\n",
      "2  Decision Tree  0.414778   0.415872  0.414778  0.411652\n",
      "3  Random Forest  0.689655   0.696677  0.689655  0.684458\n",
      "4    Naive Bayes  0.348768   0.388866  0.348768  0.310005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\Desktop\\נתניה\\HebrewHandwrittenRecognition\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "svm_accuracy = accuracy_score(y_test, svm_predictions)\n",
    "knn_accuracy = accuracy_score(y_test, knn_predictions)\n",
    "dt_accuracy = accuracy_score(y_test, dt_predictions)\n",
    "rf_accuracy = accuracy_score(y_test, rf_predictions)\n",
    "nb_accuracy = accuracy_score(y_test, nb_predictions)\n",
    "\n",
    "svm_precision = precision_score(y_test, svm_predictions, average='weighted')\n",
    "knn_precision = precision_score(y_test, knn_predictions, average='weighted')\n",
    "dt_precision = precision_score(y_test, dt_predictions, average='weighted')\n",
    "rf_precision = precision_score(y_test, rf_predictions, average='weighted')\n",
    "nb_precision = precision_score(y_test, nb_predictions, average='weighted')\n",
    "\n",
    "svm_recall = recall_score(y_test, svm_predictions, average='weighted')\n",
    "knn_recall = recall_score(y_test, knn_predictions, average='weighted')\n",
    "dt_recall = recall_score(y_test, dt_predictions, average='weighted')\n",
    "rf_recall = recall_score(y_test, rf_predictions, average='weighted')\n",
    "nb_recall = recall_score(y_test, nb_predictions, average='weighted')\n",
    "\n",
    "svm_f1 = f1_score(y_test, svm_predictions, average='weighted')\n",
    "knn_f1 = f1_score(y_test, knn_predictions, average='weighted')\n",
    "dt_f1 = f1_score(y_test, dt_predictions, average='weighted')\n",
    "rf_f1 = f1_score(y_test, rf_predictions, average='weighted')\n",
    "nb_f1 = f1_score(y_test, nb_predictions, average='weighted')\n",
    "\n",
    "performance_metrics = pd.DataFrame({\n",
    "    'Algorithm': ['SVM', 'KNN', 'Decision Tree', 'Random Forest', 'Naive Bayes'],\n",
    "    'Accuracy': [svm_accuracy, knn_accuracy, dt_accuracy, rf_accuracy, nb_accuracy],\n",
    "    'Precision': [svm_precision, knn_precision, dt_precision, rf_precision, nb_precision],\n",
    "    'Recall': [svm_recall, knn_recall, dt_recall, rf_recall, nb_recall],\n",
    "    'F1-Score': [svm_f1, knn_f1, dt_f1, rf_f1, nb_f1]\n",
    "})\n",
    "\n",
    "print(performance_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test on a manual set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing folder: 0\n",
      "Processing folder: 1\n",
      "Processing folder: 2\n",
      "Processing folder: 3\n",
      "Processing folder: 4\n"
     ]
    }
   ],
   "source": [
    "X_manual = []\n",
    "y_manual = []\n",
    "\n",
    "path = \"../data/dataset/manualTest/\"\n",
    "for folder in os.listdir(path):\n",
    "    folder_path = os.path.join(path, folder)\n",
    "    if not os.path.isdir(folder_path):\n",
    "        continue\n",
    "    if folder == \"27\":\n",
    "        continue\n",
    "    print(f\"Processing folder: {folder}\")\n",
    "    for file in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, file)\n",
    "        if os.path.isfile(file_path):\n",
    "            try:\n",
    "                img = Image.open(file_path)\n",
    "                binary_array = convert_img_to_binary_array(img)\n",
    "                X_manual.append(binary_array)\n",
    "                y_manual.append(int(folder))\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing file {file_path}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_manual' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./svm_model.sav\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[0;32m      4\u001b[0m     svm_model \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(file)\n\u001b[1;32m----> 5\u001b[0m manual_predictions \u001b[38;5;241m=\u001b[39m svm_model\u001b[38;5;241m.\u001b[39mpredict(\u001b[43mX_manual\u001b[49m)\n\u001b[0;32m      6\u001b[0m manual_accuracy \u001b[38;5;241m=\u001b[39m accuracy_score(y_manual, manual_predictions)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mManual Test Accuracy:\u001b[39m\u001b[38;5;124m\"\u001b[39m, manual_accuracy)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_manual' is not defined"
     ]
    }
   ],
   "source": [
    "#show manuak test accuracy\n",
    "import pickle\n",
    "with open('./svm_model.sav', 'rb') as file:\n",
    "    svm_model = pickle.load(file)\n",
    "manual_predictions = svm_model.predict(X_manual)\n",
    "manual_accuracy = accuracy_score(y_manual, manual_predictions)\n",
    "print(\"Manual Test Accuracy:\", manual_accuracy)\n",
    "print(\"Manual Test Predictions:\", manual_predictions)\n",
    "print(\"Manual Test Actual:\", y_manual)\n",
    "\n",
    "#predict the 4th image\n",
    "img = Image.open(\"../data/dataset/manualTest/1/1_01.png\")\n",
    "img.show()\n",
    "binary_array = convert_img_to_binary_array(img)\n",
    "prediction = svm_model.predict([binary_array])[0]\n",
    "print(\"Prediction:\", prediction)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
